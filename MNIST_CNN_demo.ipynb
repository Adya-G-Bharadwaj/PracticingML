{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**MNIST Classification Demo**\n",
        "\n",
        "This notebook demonstrates one of the most fundamental tasks in artificial intelligence: classification. The code below will download one of the most popular datasets containing images of hand-written numbers. The data is separated into 2 subsets. The \"training\" data will be used to develop a mathematical model that can successfully distinguish between the written numbers (in this case, a convolutional neural network or \"CNN\"). The \"test\" data will be used to evaluate whether the model can successfully identify each number when given new data that was not considered during the training process.  "
      ],
      "metadata": {
        "id": "KjJ2AJo3AiiH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7hAdzS7PtTtu"
      },
      "outputs": [],
      "source": [
        "# Gather the necessary tools from reliable public libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow.keras as keras\n",
        "from keras import Model\n",
        "from keras.models import Sequential\n",
        "import keras.layers as layers\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset. It comes split into two subsets for training and test.\n",
        "(training_data, training_labels), (test_data, test_labels) = keras.datasets.mnist.load_data()\n",
        "training_data, training_labels = shuffle(training_data, training_labels)\n",
        "test_data, test_labels = shuffle(test_data, test_labels)\n",
        "\n",
        "# subset the data so training doesn't take too long\n",
        "training_data = training_data[:10000]\n",
        "training_labels = training_labels[:10000]\n",
        "test_data = test_data[:1000]\n",
        "test_labels = test_labels[:1000]\n",
        "\n",
        "# Print the shapes of the loaded data\n",
        "print(\"Number of images in training data: \", len(training_data))\n",
        "print(\"Number of images in test data: \", len(test_data))\n",
        "print(\"Shape of each image:\", training_data[0].shape)"
      ],
      "metadata": {
        "id": "MqFn6OlUxygV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display 3 random images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.imshow(training_data[np.random.randint(0, len(training_data))], cmap='gray')"
      ],
      "metadata": {
        "id": "K4ItSgNPwB3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a neural network model\n",
        "\n",
        "n_epochs = 10  # number of times to cycle through the data\n",
        "batch_size = 128  # number of images used for each update to the model\n",
        "verbose = 1  # how much information should the model print during training\n",
        "\n",
        "training_data.reshape(training_data.shape[0], 28, 28, 1)\n",
        "test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "model = Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy', # use 'categorical_crossentropy' if using one-hot encoded labels\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model.summary()  # uncomment to print details of the model\n"
      ],
      "metadata": {
        "id": "UQekaqXxwkvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "start = time.perf_counter()\n",
        "m = model.fit(training_data, training_labels,\n",
        "                 epochs=n_epochs,\n",
        "                 batch_size=batch_size,\n",
        "                 verbose=verbose)\n",
        "stop = time.perf_counter()\n",
        "\n",
        "print(\"Training time: \" + str(round((stop - start) / 60, 2)) + \" minutes\")\n"
      ],
      "metadata": {
        "id": "1LHHNxMrzaYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model's performance on the test set\n",
        "loss, acc = model.evaluate(test_data, test_labels)\n",
        "print(\"Model accuracy: \" + str(round(acc, 4)))"
      ],
      "metadata": {
        "id": "F9Oxtm0y_HkR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}