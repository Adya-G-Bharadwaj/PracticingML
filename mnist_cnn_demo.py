# -*- coding: utf-8 -*-
"""Copy of MNIST_CNN_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BP6Akk-y6WNQeuIOUNcJQkCkC8IFg2br

**MNIST Classification Demo**

This notebook demonstrates one of the most fundamental tasks in artificial intelligence: classification. The code below will download one of the most popular datasets containing images of hand-written numbers. The data is separated into 2 subsets. The "training" data will be used to develop a mathematical model that can successfully distinguish between the written numbers (in this case, a convolutional neural network or "CNN"). The "test" data will be used to evaluate whether the model can successfully identify each number when given new data that was not considered during the training process.
"""

# Gather the necessary tools from reliable public libraries
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
import tensorflow.keras as keras
from keras import Model
from keras.models import Sequential
import keras.layers as layers
import numpy as np
import time

# Load the MNIST dataset. It comes split into two subsets for training and test.
(training_data, training_labels), (test_data, test_labels) = keras.datasets.mnist.load_data()
training_data, training_labels = shuffle(training_data, training_labels)
test_data, test_labels = shuffle(test_data, test_labels)

# subset the data so training doesn't take too long
training_data = training_data[:10000]
training_labels = training_labels[:10000]
test_data = test_data[:1000]
test_labels = test_labels[:1000]

# Print the shapes of the loaded data
print("Number of images in training data: ", len(training_data))
print("Number of images in test data: ", len(test_data))
print("Shape of each image:", training_data[0].shape)

# Display 3 random images
plt.figure(figsize=(10, 10))
for i in range(3):
    plt.subplot(1, 3, i + 1)
    plt.imshow(training_data[np.random.randint(0, len(training_data))], cmap='gray')

# Define a neural network model

n_epochs = 10  # number of times to cycle through the data
batch_size = 128  # number of images used for each update to the model
verbose = 1  # how much information should the model print during training

training_data.reshape(training_data.shape[0], 28, 28, 1)
test_data.reshape(test_data.shape[0], 28, 28, 1)
input_shape = (28, 28, 1)

model = Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(32, activation='relu'),
    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy', # use 'categorical_crossentropy' if using one-hot encoded labels
              metrics=['accuracy'])

# model.summary()  # uncomment to print details of the model

# Train the model
start = time.perf_counter()
m = model.fit(training_data, training_labels,
                 epochs=n_epochs,
                 batch_size=batch_size,
                 verbose=verbose)
stop = time.perf_counter()

print("Training time: " + str(round((stop - start) / 60, 2)) + " minutes")

# Evaluate the model's performance on the test set
loss, acc = model.evaluate(test_data, test_labels)
print("Model accuracy: " + str(round(acc, 4)))